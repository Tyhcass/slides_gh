---
title: "Learning Latent Variable Analysis with Dr. Hu (I)"
subtitle: "潜在变量分析I"
author: "胡悦"
institute: "清华大学政治学系"
# date: "2019-06-01"
output:
  xaringan::moon_reader:
    lib_dir: libs
    css: 
      - zh-CN_custom.css
      - styles.css
      - "https://use.fontawesome.com/releases/v5.6.0/css/all.css"
      - "https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.7.0/animate.min.css"
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)

library(pacman)

p_load("psych", "lavaan", "semPlot", "qgraph", "haven", "corrplot",  "tidyverse")
```


## 内容概要

* 潜在变量分析概述
* 什么是EFA
    + EFA诊断
    + EFA vs. PCA
* 什么是CFA
    + CFA诊断
* SEM 入门

---

## 操作语言

* R
    + [`psych`](https://personality-project.org/r/psych/vignettes/intro.pdf)
    + [`laavan`](https://lavaan.ugent.be/)

---

class: inverse, bottom

# 潜在变量分析：一种方法

---

## 潜在变量

.left-column[
<img src="images/LoveLife.jpg" height = 400 />
]

???

Anna Kendrick's 2020 show, showing how difficult to find "the one." This "being loved" is a latent variable.

--

.right-column[

1. 不可见(Unobservable)
1. 多维度(Multidimensional)
1. 有效果(Consequential)

]

???

The latent variable per se can't be directly measured. But its consequences in opinions and behaviors can be observed.

---

## 为什么需要了解潜在变量？

公共舆论、政治文化、政治心理……几乎所有社会科学学科：

* 抽象
* 复杂
* 综合

--

".magenta[Cornerstone] of successful scientific inquiry."  
---Delli Carpini and Keeter 1993

???

Delli Carpini, M. X., and S. Keeter. 1993. “Measuring Political Knowledge: Putting First Things First.” American Journal of Political Science 37 (4): 1179–1206.

---

## 怎么找？

例：测量个体的“社会资本”（social capital）

指标问题X（1~10）：

1. 您是否信任身边人？
1. 您在政府机关有没有亲戚
1. 您的朋友是否和您的想法经常一致？

$$\tilde{X} = (X_1 + X_2 + X_3)/3.$$

累加型综合法(additive scales)

---

看起来可能有用……

--

1. 平等权重(equal weight)
1. 数据敏感(extreme value sensitivity)
1. 忽略极化(polarity ignoring)

---

## "易求无价宝，难得有心郎！"

### 因素分析模型(Factorial Models)

1. 探索性因子分析(EFA)
1. 验证性因子分析(CFA)
1. 结构方程模型(SEM)

### 类型分析模型(Typological Models)


1. 项目反应理论(IRT)
1. 跨群组项目反应(MrP, GIRT, DCPO)

???

鱼玄机《赠邻女》：羞日遮罗袖，愁春懒起妆。易求无价宝，难得有心郎

---

class: inverse, bottom

# 因素分析综述

---

## 因素分析模型(Factorial Models)

1. 探索性因子分析(EFA)
1. 验证性因子分析(CFA)
1. 结构方程模型(SEM)

---

### 理论

.megenta[可见]指标(indicators).magenta[&larr;]潜在变量

???

uncover the underlying structure of a relatively large set of variables. 

--

.left-column[
### 操作

.magenta[Minimum] factors for the variances

a.k.a. 变量简化  
(降维打击？)
]

.right-column[
<img src="images/jiangwei.jpg" height = 300 />
]

???

determine the minimum number of hypothetical factors or components that account for the variance between variables.

---

## Like...How?

.center[<img src="images/efa.png" height = 500 />]

???

Known: I wanna fewer variables
Unknown: how many? how to combine?

---

class: inverse, bottom

# 探索性因子分析
## Exploratory Factor Analysis

---

## 理念背后

.center[.large[**X<sup>*</sup> = &Phi;&Lambda;' + &delta;**]]


**X<sup>*</sup>**: 潜在变量  
**&Phi;**: 指标选择  
**&Lambda;**: 单项贡献（a.k.a., factor loading）  
**&delta;** 选择误差

???

Quinn, Kevin M. undefined/ed. “Bayesian Factor Analysis for Mixed Ordinal and Continuous Responses.” Political Analysis 12(4): 338–53.

---

## 操作实现

1. 因子个数选择
1. Rotation
1. 因子合成
1. 结果检验

---

## 一个心理学案例

19,719 参与者, the "Big Five Personality" Test

变量名：
```{r data-big5}
df_big5 <- read_csv("data/dataBIG5.csv")
df_big5[df_big5 == 0] <- NA
cat(names(df_big5))
```

???

经验开放性Openness, 尽责性Conscientiousness, 外向型Extraversion, 亲和性Agreeableness, 情绪不稳定型Neuroticism

https://quantdev.ssri.psu.edu/tutorials/intro-basic-exploratory-factor-analysis

---

## 因子个数选择

### 相关性分析

```{r corrplot, fig.align='center', fig.height=5.5}
df_big5 %>%
  select(8:57) %>%
  cor(use = "complete.obs") %>%
  corrplot(order = "hclust",
           tl.col = 'black',
           tl.cex = .75) 
```

---

### Horn's Parallel Analysis

已知观测数据集**O**<sub>m&times;n</sub>

1. 创建随机数据集**R**<sub>m&times;n</sub>;  
1. 相关矩阵<sub>**R**</sub> 和 &lambda;<sub>**R**</sub>;  
1. 相关矩阵<sub>**Ok**</sub> 和 &lambda;<sub>**Ok**</sub> (k为因子数)
1. &lambda;<sub>**Ok**</sub> vs. &lambda;<sub>**R**</sub>

A. 如果&lambda;<sub>**Ok**</sub> < &lambda;<sub>**R**</sub>, 则 ~~k~~  
B. Kaiser criterion

???

&lambda;是相关矩阵特征值
Kaiser criterion: 特征值<1 不可

---

```{r parallelAnalysis, fig.align='center', fig.height=8, results='hide'}
df_big5 %>%
  select(8:57) %>%
  fa.parallel(fa = "fa")
```

---

## 因子提取(Factor Extraction)

+ .magenta[Minimum residual (OLS)]
+ Principal axes
+ Alpha factoring
+ Weighted least squares
+ Minimum rank
+ .magenta[Maximum likelihood (ML, minimum &chi;<sup>2</sup> goodness of fit)]

---

## Rotation

.center[<img src="images/rotation.jpg" height = 530 />]

???
Rotation:

A pattern of loadings where each item loads strongly on only one of the factors, and much more weakly on the other factors.

---

简化数据结构

* Orthogonal: Varimax, quartimax, bentlerT, geominT, bifactor  
* .magenta[Oblique]: Oblimin, quartimin, simplimax, bentlerQ, geominQ, biquartimin


???

Orthogonal(正交)

优选Oblique，允许factor间correlate

---

## 因子合成

```{r efa}
result_efa <- df_big5 %>%
  select(8:57) %>%
  fa(nfactors = 5,
     rotate = "oblimin",
     fm = "minres")

print(result_efa$loadings, cutoff = .296) # the point was picked for the purpose of presentation
```

---

## 成了吗？

### 常见诊断指标

1. Sum of squared (SS) loading<sup>1</sup>
1. Communality & Uniqueness
1. Root means square of residuals(RMSR)
1. Tucker-Lewis Indes (TLI)
1. Reliability test (Crobach's &alpha;)<sup>2</sup>

.footnote[
[1] 特征值  
[2] 为每个因子分别计算
]

???

SS：Kaiser criterion: &lambda; < 1 不可
Communality:  SS of all the factor loadings for a given variable
Uniqueness: 1 - Communality
RMSR < 0.05
TLI > 0.9

Crobach's &alpha;: 计算因子分布的variables是不是内部一致，除非发表，不大必要

---

```{r efa-full}
result_efa
```

---

## 因子在哪里？

"score"

```{r scores, fig.align='center', fig.height=6}
df_score <- result_efa$scores %>% 
  as_tibble()
hist(df_score$MR1)
```

---

## Principal Component Analysis

除EFA外的另一选择

--


.center[<img src="images/pca.png" height = 300 />]



.large[.center[C = w<sub>i</sub>Y<sub>i</sub>]]

???

create one or more index variables (Components) from a larger set of measured variables (Y)

---

## PCA vs. EFA

结果或近似，逻辑大不同

.center[<img src="images/pcaVsEfa.PNG" height = 400 />]

???
PCA: measurement to index   

write all variables in terms of a smaller set of features which allows for a maximum amount of variance to be retained in the data.   


EFA: indices to measurement (of a latent variable)   

find a set of features which allow for understanding as much of the correlations between measured variables as possible. individually.

---

## 怎么选

--

* PCA最大程度保留可见变量信息，EFA旨在提取不可变量特征；

--

* 当Variable之间关系不那么紧密或受同一变量影响，PCA > EFA；

--

* 当估计潜在变量时，PCA可能夸大可见指标的影响

---

class: inverse, bottom

# 验证性因子分析
## Confirmative Factor Analysis

---


## 探索性 vs 验证性：实现

.left-column[
### EFA: 数据指向

1. 实证观察（归纳）
1. 未知维度
    + 每个维度都对指标产生影响
1. 多重指标
    + Loading大小之分
1. 无视测量偏差关联
1. Underidentified

]

.right-column[
### CFA: 理论指向

1. 理论定义（演绎）
1. 明确维度
    + 维度-指标关系明确
1. 单一指标
    + Loading有无之分
1. 允许测量误差相关
1. 必须identifiable
]

---

## CFA 指标选择

1. 严格依据理论
1. 每维度一指标

--

### 例：人际信任

定义：.magenta[相信]他人不会.magenta[伤害自己]或.magenta[违背约定].

--

指标：
.small[
1. 总体而言，您是认同多数人是值得信任的，还是防人之心不可无？
1. 您认为多数时候人们是乐于助人的还是自私自利的？
1. 如果有机会，您认为别人是会占您便宜，还是说会恪守约定、公平行事？
]

---

## CFA 指标选择

操作原则：

1. 每个维度一个指标，但维度拆分.magenta[艺术>技术]

--

1. 效果.magenta[~~原因~~]指标

--

```{r causal, fig.align='center', fig.height=5}
DiagrammeR::grViz(
  "digraph {

graph[layout = dot]

node[shape = rectangle, style = filled, fillcolor = Linen]

s [label = '压力', shape = 'ellipse']
d [label = '家人\n去世']
j [label =  '丢失\n工作']
i [label = '身患\n疾病']

# edge definitions with the node IDs
s -> {d, j, i}
}"
)
```

???
CFA多数针对效果指标，因此上述例子不好

---

## 模型建构

.center[<img src="images/multitrait.png" height = 550 />]

???

裁判对滑雪运动员的评判

单侧: factor complexity = 1 （一个变量只贡献一个latent）  
双侧: factor complexity > 1

---

.center[.large[**X = &Lambda;<sub>X</sub>&xi; + &delta;**]]

**X**： 指标向量  
**&xi;**：潜在变量  
**&Lambda;<sub>X</sub>**: X = f(&xi;)系数(a.k.a., loading, path)  
**&delta;**：偏误向量  

**&Phi;**: 潜在变量的协方差矩阵  
**&Theta;<sub>&sigma;</sub>**: 偏误的协方差矩阵

???

**&xi;**: ksi  
**&delta;**: delta  
**&Phi;**: phi  
**&Theta;<sub>&sigma;</sub>**: theta  

在EFA中潜在变量在等号左边，观测变量在右边

---

.center[<img src="images/partMultitraitpng.png" height = 300 />]

$$\begin{bmatrix}x_1\\
x_2\\
x_3\\
x_4\\
x_5
\end{bmatrix} = \begin{bmatrix}1 & 1 & 0\\
\lambda_{21} & 1 & 0\\
\lambda_{31} & 1 & 0\\
\lambda_{41} & 0 & 1\\
\lambda_{51} & 0 & 1
\end{bmatrix} \begin{bmatrix}\xi_1\\
\xi_2\\
\xi_3
\end{bmatrix} + \begin{bmatrix}\delta_1\\
\delta_2\\
\delta_3\\
\delta_4\\
\delta_5
\end{bmatrix}$$

---

潜在变量矩阵

**&Phi;** = 
\begin{bmatrix}
\phi_{11} & & \\
0 & \phi_{22} & \\
0 & \phi_{23} & \phi_{33}
\end{bmatrix}


偏误矩阵

diag **&Theta;<sub>&sigma;</sub>** = diag[var(&delta;<sub>1</sub>) var(&delta;<sub>2</sub>)...var(&delta;<sub>5</sub>)]

???

&phi;23$: classical 和showy 裁判的相关性

---

## 你最多能连多少线？：Identification

\begin{cases}
y = x + z\\
x = 2y - 3z
\end{cases}

--

当**&Lambda;、&Phi;、&Theta;**存在唯一解时，模型是identified.


&Lambda;: $X = f(\xi)$系数(a.k.a., loading)  
&Phi;: 潜在变量的协方差矩阵  
&Theta; 偏误的协方差矩阵

--

**t rule**:

t < q(q + 1)/2

t: 不可见
q：可见

???

Another way to calculate df.

---

为保证identifable， 要对CFA进行限制

1. Scaling
    + &lambda;<sub>11</sub> = 1;
    + 将潜在变量方差设为1(Standardized metric)
2. 参数调整
    + 特定因子loading设为0；
    + 偏误的协方差设为0；
    + 偏误的方法吃设为0；

---

## 解方程

**&Sigma;**: 所有可见指标得协方差矩阵

.center[.large[**&Sigma;(&theta;) = &Lambda;<sub>X</sub>&Phi;&Lambda;<sub>X</sub>' + &Theta;<sub>&delta;</sub>**]]

.left-column[
全信息估计：

Maximum likelihood:   
Generalized Least Squares  
Unweighted Least Squares
]

.right-column[
有限信息估计：

Two Stage Least Squares

]

???

寻找最优**&Sigma;**

---

## 常见问题
### Matrix to be analyzed not positive definite

+ Typo in data file or covariance matrix
+ Pairwise deletion of missing data
+ Perfect collinearity
+ Outliers/influential cases

---

### Improper solutions

+ Typo in program
+ Population value near the borderline combined with sampling fluctuation
+ Specification error
+ Small samples (<150 or so) combined with 2 indicators per factor
+ “unlucky” sample
+ Outliers/influential cases

---

### Nonconvergence

+ Typo in program, data file, or covariance matrix
+ Underidentified model
+ Poor model specification
+ Bad starting values
+ Small samples (N<100)
+ Only 2 indicators per factor
+ Outliers/influential cases
+ Observed variables in very different scale

---

## 成了吗？

### Overall: &chi;<sup>2</sup>

结果显著则说明整体模型可能有问题。

---

然而，当N足够大时&chi;<sup>2</sup>通常显著

???

1. Because models are almost always incorrect, any otherwise reasonably fitting model can be rejected when sample size is sufficient
1. Also a poor fitting model could be accepted if there is insufficient power to detect misspecification.

---

### Incremental Fit Indices

将模型与基线模型比较

???

基线模型：只保留可见变量的方差和协方差

--

#### Tucker-Lewis Index (TLI, &rho;<sub>2</sub>, Non-Normed Fit Index)

#### Comparative Fit Index (CFI)

+ 1 理想情况
+ <.95 不可接受

???

CFI可以>1, 这种情况视为1

---

### Absolute Fit Indices

#### Root Mean Square Error of Approximation (RMSEA)
#### Standardized Root Mean Square Residual (SRMR)

+ 0 理想情况
+ &le; 0.05 不错
+ 0.05 ~ 0.08 差强人意
+ &ge; 0.1 不可接受

---

## 实例：Holzinger & Swineford 1939

.left-column[
对初中生精神状况的调查：
+ 视觉因素：x<sub>1~3</sub>
+ 阅读因素：x<sub>4~6</sub>
+ 表达因素：x<sub>7~9</sub>
]

.right-column[
```{r diagram-cfa, fig.align='center', fig.height=4}
HS.model <- ' visual  =~ x1 + x2 + x3
              textual =~ x4 + x5 + x6
              speed   =~ x7 + x8 + x9 '
fit <- cfa(HS.model, data = HolzingerSwineford1939)


semPaths(
  fit,
  layout = "tree",
  curve = 1,
  rotation = 4,
  nCharNodes = 0,
  mar = c(3, 20, 3, 20),
  border.width = 1.0,
  esize = 1.0,
  edge.color = "black",
  label.scale = FALSE,
  label.cex = 1.0,
  residuals = FALSE,
  fixedStyle = 1,
  freeStyle = 1,
  curvePivot = FALSE,
  sizeMan = 7,
  sizeLat = 10
)
```
]

---

```{r results-hsModel}
summary(fit, fit.measures=TRUE)
```

---

class: inverse, bottom

# 结构方程模型
## Structural Equation Model

---

## 结构方程模型

+ Structural equation models (SEMs)
+ LISREL (Linear Structural ReLationships) models
+ Covariance structure models
+ Latent variable models
+ Structural equations with latent variables

---

## “老朋友”

Confirmative factor analysis  
Multiple regression  
Multivariate regression  
ANOVA  
General linear model  
Path analysis  
Recursive models  
Dichotomous and ordered probit  
Seemingly unrelated regressions  
Simultaneous models  
Latent growth curve models  
......

---

## CFA之上

与CFA相比，SEM：

+ 观察变量： X, .magenta[Y]

--

+ 模型建构：
    1. Latent variable model
    1. Measurement model (CFA)
    1. Relations among the errors 
    
--

+ 依然要求identification

---

## 估计

MLE

---

## 诊断

+ Overall：&chi;<sup>2</sup>
+ Incremental：
    + TLI & CLI
    + Incremental Fit Index(IFI, &Delta;<sup>2</sup>): 1理想，<0.9不可接受
+ Absolute:
    + RMSEA: < 0.06
    + BIC: 越小越好
    
---

实例：Bollen 1989

政治民主（1960，1965）与工业化

```{r diagram-sem, fig.align='center', fig.height=4, fig.keep="last"}

model <- '
  # measurement model
    ind60 =~ x1 + x2 + x3
    dem60 =~ y1 + y2 + y3 + y4
    dem65 =~ y5 + y6 + y7 + y8
  # regressions
    dem60 ~ ind60
    dem65 ~ ind60 + dem60
  # residual correlations
    y1 ~~ y5
    y2 ~~ y4 + y6
    y3 ~~ y7
    y4 ~~ y8
    y6 ~~ y8
'
fit <- sem(model, data = PoliticalDemocracy)
Graph <- semPaths(
  fit,
  #layout=L,
  layout = "tree2",
  nCharNodes = 0,
  curve = 1,
  rotation = 4,
  #mar=c(3,3,3,3),
  edge.color = "black",
  border.width = 1.0,
  esize = 1.0,
  label.scale = FALSE,
  label.cex = 1.0,
  residuals = FALSE,
  fixedStyle = 1,
  freeStyle = 1,
  curvePivot = FALSE,
  #style="LISREL",
  exoVar = FALSE,
  sizeMan = 7,
  sizeLat = 10,
  DoNotPlot = FALSE
)
L <- matrix(
  c(
    0.4 ,
    1 ,
    0.7 ,
    1 ,
    1.0 ,
    1 ,
    -1.0 ,
    1 ,
    -1.0 ,
    0.71 ,
    -1.0 ,
    0.43 ,
    -1.0 ,
    0.14 ,
    -1.0 ,
    -0.14 ,
    -1.0 ,
    -0.43 ,
    -1.0 ,
    -0.71 ,
    -1.0 ,
    -1    ,
    0.7 ,
    0.4 ,
    0.0 ,
    0.4 ,
    0.0 ,
    -0.4
  ),
  14,
  2,
  byrow = TRUE
)
qgraph(Graph, layout = L, arrowAngle = 0.5)
```

---

```{r results-semModel}
summary(fit, fit.measures=TRUE)
```

---

## 在操作SEM之前，你可能还想知道

1. SEM怎么对待Measurement errors
1. SEM怎么处理非线性变量(hint: GSEM)
1. SEM怎么对待群组效应
1. SEM如何处理缺失值(hint: 可能比MI还要好哦！)

--

1. .magenta[SEM 不能证明因果关系！！]

---

## 总结一下

* 潜在变量分析概述
* 什么是EFA
    + EFA诊断
    + EFA vs. PCA
* 什么是CFA
    + CFA诊断
* SEM 入门

---

* 潜在变量分析概述
    + .magenta[因素分析]
    + 类型分析
* 什么是EFA
    + 探索性因子分析：通过loading找到潜在变量
    + EFA诊断：Kaiser‘s criterion, reliability
    + EFA vs. PCA: 结果相似，逻辑不同
* 什么是CFA
    + 验证性因子分析：检验潜在变量对于观察指标是否有影响
    + CFA诊断：&chi;<sup>2</sup>, TLI/CFI，RMSEA/SRMR
* SEM 入门
    + Latent + measurement models
    + 不能证明因果关系！

---

class: inverse, center, middle

# Questions, Comments, and Suggestions?