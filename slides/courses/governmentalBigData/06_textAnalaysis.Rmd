---
title: "计算机辅助文本分析基础篇"
subtitle: "Learning Basic Text Analysis with Dr. Hu"
author: "胡悦<br>清华大学政治学系"
# date: "2019-06-01"
output:
  xaringan::moon_reader:
    chakra: libs/remark-latest.min.js
    css: 
      - default
      - zh-CN_custom.css
      - styles.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(echo = FALSE,
                      message = FALSE,
                      warning = FALSE)

# wrap hook

library(knitr)
hook_output <- knit_hooks$get('output')
knit_hooks$set(output = function(x, options) {
  # this hook is used only when the linewidth option is not NULL
  if (!is.null(n <- options$linewidth)) {
    x = knitr:::split_lines(x)
    # any lines wider than n should be wrapped
    if (any(nchar(x) > n)) x = strwrap(x, width = n)
    x = paste(x, collapse = '\n')
  }
  hook_output(x, options)
})


library(pacman)

p_load("tidyverse", "stringr", "rvest", "tidytext", "jiebaR")
```


## Overview

### 理论

* Text as Data: 文本与文本分析
* 文本分析的基本原则

### 操作

* 文本获取
* 文本整理
* 基本分析
* （进阶分析）

---

## 技术来源

* 平台：R
    + [`rvest`](https://rvest.tidyverse.org/)
    + [`tidytext`](https://www.tidytextmining.com/)
    + [`jiebaR`](https://qinwenfeng.com/jiebaR/)
    
--

* 其他平台：
    + Python
    + C++
    + Java

---

class: inverse, bottom

# 文本数据与分析概况

---

## Text as Data

每年： 2017-2018年年增量56,530,000人。

--

每天：
+ 百度日用户搜索请求，需1.7天才能扫描一遍；
+ 微信日增数据500TB——比人类所有书籍存量还多。

--

每秒：全世界每秒发送290万封电子邮件，一个人需要5.5年日以继夜才能读完。

--

### 1080p HD 2hrs：6GB

--

2019年，数据总量达40ZB，人均5.2TB<sup>1</sup>

.footnote[<sup>1</sup> 1ZB = 1,024EB = 1,024<sup>2</sup>PB = 1,024<sup>3</sup>TB  = 1024<sup>4</sup>GB.]

???

Giga Byte < Tera Byte < Peta Byte < 
Exa Byte < Zetta Byte < Yotta Byte

---

## 文本之于社会科学

历史悠久而非主流
+ 资料难获取；花时间；难推广；难管理；难分析

--

新兴工具的繁荣：
+ 文本.navy[资料]指数级增长；
+ 大规模文本数据.navy[采集]；
+ .navy[存储和管理]能力增强；
+ 文本分析.navy[方法]蓬勃发展：
+ 可推广、系统化和.navy[廉价]化；
+ 通过文本表达的社会意义更.navy[广泛]。

---

## 社会科学中的文本分析

研究者.navy[描述和阐释]一系列记录或可视文本的方法。

--

符号化 + 标准化

???

存在；密度；频次

结构；功能


---

## 计算机辅助文本分析

* Text analysis vs. content analysis

--

* Representational analysis vs. Instrumental analysis

???

精确解码，关注外显（manifest）；
探索意图，关注隐含（latent）

--

* Thematic analysis vs. semantic analysis

???

概念是否出现及何种关系，基于词频和向量；
识别主题间的具体关系；考虑语法、逻辑等

语法学(syntax, how to say it)
语义学(semantic, what is said)
语用学(pragmatic, what is implicated)

---

## 分析对象

.pull-left[

~~文字~~ 语言

 Language matters!

]

--

.pull-right[

<img src="images/text_arrival.jpg" height = 450 />

]

---

## 分析挑战

* 非结构化

--

* 海量潜在维度

--

* 语言复杂且微妙

--

* 过载、失实、 冗余、污染


---

## 分析实例

Grimmer 2010. 

* 目标：美国参议员的政治沟通

???

Grimmer, Justin. 2010. “A Bayesian Hierarchical Topic Model for Political Texts: Measuring Expressed Agendas in Senate Press Releases.” *Political Analysis* 18(1): 1–35.

--

* 核心方法：贝叶斯分层主题模型

--

* 数据：美国参议院2007年来发布的24000余份新闻通告

--

* 发现：
    + 关注重点与其他参议员的关注事件相关；
    + 关注重点的地域分布具集聚性；
    + 议员对挪用的关注程度与他们对禁止挪用法案的投票呈现出正相关关系。


---

## Benoit et al. 2016. 

* 问题：专家与普通人对政治理解异同
* 方法：Crowd-sourced identification
* 数据：18,263 natural sentences, 1987~2010
    + 215,107 群众评判
    + 123,000 专家评判

???

Benoit, Kenneth et al. 2016. “Crowd-Sourced Text Analysis: Reproducible and Agile Production of Political Data.” *American Political Science Review* 110(2): 278–95.

British Conservative, Labour and Liberal Democrat manifesto, 

---

<img src="images/text_crowd-sourced.jpg" height = 450 />


---

<img src="images/text_expert-crowd.png" height = 400 />


---

## Dietrich et al 2019 

* 目标： 测量情感强烈度
* 数据：74,158 Congressional floor speeches
* 方法： 统计音高变化

???

Dietrich, Bryce J., Matthew Hayes, and Diana Z. O’Brien. 2019. “Pitch Perfect: Vocal Pitch and the Emotional Intensity of Congressional Speech.” American Political Science Review: Forthcoming.

--

.center[<img src="images/text_vocalPitch_hcy.png" height = 270 />]

---

<img src="images/text_vocalPitch.png" height = 600 />


???

female MCs speak with greater emotional intensity when talking about women as compared to both their male colleagues and their speech on other topics.


---

## Zhang & Pan 2019

* 目标：从社交媒体中获取集体行动信息
* 数据：A random sample of 20,000 geocoded posts from Weibo, 2010--2017
* 方法：CNN for images; CNN-RNN for texts


???

Zhang, Han, and Jennifer Pan. 2019. “CASM: A Deep-Learning Approach for Identifying Collective Action Events with Text and Image Data from Social Media.” Sociological Methodology 49(1): 1–57.

张涵（普林斯顿）

convolutional neural network (CNN) for image classification and a combination of convolutional and recurrent neural networks with long short-term memory (CNN-RNN)

---

background-image: url(images/text_casm.png)
background-position: 50% 50%
background-size: 100%

---

Validation

<img src="images/text_runningMan.png" height = 250 />
<img src="images/text_banner.png" height = 250 />

---

class: inverse, bottom

# 文本分析操作

---

## 文本资源: 原生数据

* Email/短信
* 网站HTML
* RSS feeds
* 网络社交媒体
* 网络论坛
* 网络问答平台
* 媒体数据库
* 网络交易行为
……

---

## 社交媒体

.center[<img src="images/text_social-media.gif" height = 400 />]

???

新浪微博1.4亿，微信用户5.5亿;
微信日增数据500TB，QQ日增数据200TB。

---

## 公共开放平台

.center[<img src="images/text_zhongsheng_index.png" height = 500 />]

---

## 网络问政平台

.center[<img src="images/text_henan-Egovernment.png" height = 400 />]

---

## 社会化问答网站

.center[<img src="images/text_zhihu.png" height = 350 />]

???

Quora, Stack Overflow

---

## 媒体数据库

.center[<img src="images/renminribao.png" height = 500 />]

---

## 问卷开放性问题

.center[<img src="images/text_survey_openQuestion.png" height = 450 />]

---

## 文本资源: 档案数据

* 中国知网等数据库（期刊、报刊、年鉴等）
* Google Books、百度学术
* Google Trend、百度指数
* JSTOR Data for Research……

---

## 百度指数

.center[<img src="images/text_baidu_zhishu.png" height = 400 />]

---

## 文本资源：数字化

* 文本扫描 + OCR
.center[<img src="images/text_hongkong_lib.png" height = 400 />]

---

.center[<img src="images/text_cbdb.png" height = 600 />]

---

## 文本获取

* 档案数据和数字化数据
* 原生数据：
    + Spider/crawler/scraper
    + 清理

---

## 抓取工具

.center[<img src="images/text_bazhuayu.png" height = 500 />]

---

.center[<img src="images/text_huochetou.png" height = 550 />]

---

.center[<img src="images/text_gooseeker.png" height = 550 />]

---

background-image: url(images/text_gooseekerI.png)
background-position: 50% 50%
background-size: 100%

---

background-image: url(images/text_gooseekerII.png)
background-position: 50% 50%
background-size: 100%

---

background-image: url(images/text_gooseekerIII.png)
background-position: 50% 50%
background-size: 100%

---

## 用R进行文本抓取

`SelectorGadget`(Chrome)

.center[<img src="images/text_zhongsheng_page.png" height = 350 />]

---

`rvest`(R)

```{r zhongsheng-scrape, eval = FALSE, include = FALSE}
ls_zhongsheng <- read_html("http://politics.people.com.cn/GB/8198/426918/index.html") %>% # index page
  html_nodes("h5 a") %>% # the nodes of the links
  html_attr("href") %>% # just the links
  str_replace("^/n1", "http://politics.people.com.cn/n1")

df_zhongsheng <- map_df(ls_zhongsheng, function(link) {
  zs_article <- read_html(link, encoding = "GB18030") # read the html
  
  zs_title <- html_nodes(zs_article, "h1") %>%
    html_text
  
  zs_time <- html_nodes(zs_article, ".box01 .fl") %>%
    html_text %>%
    str_extract("\\d{4}年\\d{2}月\\d{2}日")
  
  zs_content <- html_nodes(zs_article, "#rwb_zw p") %>%
    html_text %>%
    str_c(collapse = "") %>% # combined the paragraphs
    str_remove_all("\\s|\\n|\\t") # remove the horizontal spaces
  
  zs_data <- data.frame(title = zs_title,
                        time = zs_time,
                        content = zs_content) %>%
    mutate(
      time = str_replace(time, "(\\d{4})年(\\d{2})月(\\d{2})日", "\\1-\\2-\\3"),
      time = as.Date(time)
    )
})

saveRDS(df_zhongsheng, "zhongsheng.RDS")
```

```{r zhongsheng-eg, eval = FALSE, echo=TRUE}
ls_zhongsheng <- read_html("http://politics.people.com.cn/GB/8198/426918/index.html") %>% # index page
  html_nodes("h5 a") %>% # the nodes of the links
  html_attr("href") %>% # just the links
  str_replace("^/n1", "http://politics.people.com.cn/n1")

df_zhongsheng <- map_df(ls_zhongsheng, function(link) {
  zs_article <- read_html(link, encoding = "GB18030") # read the html
  
  zs_title <- html_nodes(zs_article, "h1") %>%
    html_text
  
  zs_time <- html_nodes(zs_article, ".box01 .fl") %>%
    html_text %>%
    str_extract("\\d{4}年\\d{2}月\\d{2}日")
  
  zs_content <- html_nodes(zs_article, "#rwb_zw p") %>%
    html_text %>%
    str_c(collapse = "") %>% # combined the paragraphs
    str_remove_all("\\s|\\n|\\t") # remove the horizontal spaces
  
  zs_data <- data.frame(title = zs_title,
                        time = zs_time,
                        content = zs_content) 
})
```

---

## Regular Expression

.center[<img src="images/text_regular_expression.png" height = 500 />]

---

## 结果

```{r zhongsheng-analyze}
df_zhongsheng <- readRDS("data/zhongsheng.RDS")
```

.center[<img src="images/text_zhongsheng_output.png" height = 300 width = 700 />]

---

## 文本分析方法与流程

* 获取数据
* 结构化
* Supervised vs. Unsupervised

---

<img src="images/text_analysis-method.png" height = 550 />

---

## 常见文本分析方法

### 描述

词云(word-cloud)、网络

### 聚类

知类分文、知文分类

### 语义

情感分析(sentiment analysis)

---

## 孤木不成林

* 互联网舆情 + 法院判决信息、股价信息等 &rArr; 行业发展前景；
* 交易流水信息 &rArr; 提高消费者画像精确度；
* 网络社交与候选人资料 &rArr; 国会选举、投票率等

---

## 基本原则

(Grimmer & Stewart 2013)

--

1. 所有现存关于语言的定量模型都是有偏误的——但不乏能提供有用的信息。

--

1. 文本定量模型旨在增强人（作为主体）的辨识范围和能力。

--

1. 不存在通行的文本分析最优模型。

--

1. 有效性！有效性！有效性！


---

## 核心假定： A Bag of Words

A text is represented as the bag (multiset) of its words. 

--

<img src="images/text_bagOfWords.png" width = 700 />


---

## 操作化: Tokenization

* 文章打散

* 洗去Syntax
    + 大小写
    + stemming (e.g., 美国式、美式)
    + 标点
    + 非字符（@#￥%……&*）

* 区别词性
    + Content vs. function
    + 名、动、形、副、介、代……

---

### Stop words


```{r stop-words-en}
read_lines(STOPPATH)[883:933]
```

--

```{r stop-words-ch}
read_lines(STOPPATH)[157:207]
```

---

常见中文stop words资源

* 百度停词表
* 哈工大停词表
* 网络搜集停词表，如[“最全中文停用词表整理（1893个）”](https://blog.csdn.net/shijiebei2009/article/details/39696571)
* 自定义

---

.center[<img src="images/text_bagOfWords2.jpg" height = 500 />]

---

class: small

## 亚洲特色：Segment

"一场经贸摩擦，让世界看到了一个有担当负责任的中国和一个一意孤行的美国。”埃及《金字塔报》资深记者萨米·卡姆哈维的话意味深长。近年来，美国一些政客被“美国优先”遮住了双眼，大搞贸易保护主义、单边主义，肆意挥舞关税大棒，全然不顾中美两国人民和全世界人民的强烈反对。"——"难道非要撞了南墙才回头——意孤行必将失败", 《人民日报》（2019年05月30日02版）


```{r segment}
zhongsheng <- "一场经贸摩擦，让世界看到了一个有担当负责任的中国和一个一意孤行的美国。”埃及《金字塔报》资深记者萨米·卡姆哈维的话意味深长。近年来，美国一些政客被“美国优先”遮住了双眼，大搞贸易保护主义、单边主义，肆意挥舞关税大棒，全然不顾中美两国人民和全世界人民的强烈反对。"
cutter <- worker() # segment engine
new_user_word(cutter, "萨米·卡姆哈维", "nr") # customize segmentation

segment(zhongsheng, cutter)
```

---

## 词频与词性分析

```{r zhongsheng-clean}
df_zhongsheng$segmented <- map_chr(df_zhongsheng$content, function(content){
  segment(content, cutter) %>% paste(collapse = " ")
})

df_zhongsheng$phase <- "US_fail"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-22"] <- "theory_test"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-11"] <- "reassessment"
df_zhongsheng$phase[df_zhongsheng$time <= "2019-05-08"] <- "optimism"


df_token <- df_zhongsheng %>% 
  select(-content) %>% 
  unnest_tokens(word, segmented) # tokenization

# Show the word counts
select(df_token, -phase) %>% head
```

--

去掉停词

```{r zhongsheng-stop}
# removing the stop words
df_stopWords <- tibble(word = read_lines(STOPPATH))

df_token <- df_token %>% 
  anti_join(df_stopWords) 

select(df_token, -phase) %>% head
```

---

```{r zhongsheng-frequency}
df_token %>% count(word, sort = TRUE) %>% 
  filter(n > 151) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  ylab("词频统计") + 
  theme(axis.text = element_text(size = 20)) +
  coord_flip()
```

---

```{r zhongsheng-phase-freq, out.width="90%"}
df_token %>% 
  group_by(phase) %>% 
  count(word, sort = TRUE) %>% 
  top_n(10) %>%
  ungroup() %>% 
  mutate(word = reorder(word, n),
         word = factor(word, levels = rev(unique(word)))) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
  ylab("词频统计") + 
  theme(axis.text = element_text(size = 18)) +
  facet_wrap(~ phase, scales = "free")
```

???

The Economist journalist Simon Rabinovitch

---

```{r zhongsheng-fre-compare, out.width="90%"}
frequency <- df_token %>% 
  count(phase, word) %>%
  group_by(phase) %>%
  mutate(proportion = n / sum(n)) %>% 
  select(-n) %>% 
  spread(phase, proportion) %>% 
  gather(phase, proportion, optimism:theory_test)

library(scales)

# expect a warning about rows with missing values being removed
ggplot(frequency, aes(x = proportion, y = `US_fail`, color = abs(`US_fail` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5, family = "WenQuanYi Micro Hei") +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~phase, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "US Inevitable Failure", x = NULL) +
  theme(text = element_text(family = "WenQuanYi Micro Hei"))
```

---

## Keyword Analysis


```{r keyword}
extractor_keyword <- worker("simhash", topn = 5)

simhash(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"], extractor_keyword)
```

---

## Hamming Distance

.left-column[
<img src="images/text_hammingDis_org.png" height = 300 />
]

.right-column[
<img src="images/text_hammingDis_comp.png" height = 300 />
]

---

05-30 vs. 05-22

```{r keyword-distnace}
distance(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"],
         df_zhongsheng$content[df_zhongsheng$time == "2019-05-22"],
         extractor_keyword)
```

05-30 vs. 05-23

```{r keyword-distance2}
distance(df_zhongsheng$content[df_zhongsheng$time == "2019-05-30"],
         df_zhongsheng$content[df_zhongsheng$time == "2019-05-23"][1],
         extractor_keyword)
```


???

the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.

---

## Topic Modeling

.center[
<video width="500" height="500" controls>
<source src="images/text_topicModeling.webm" type="video/webm">
</video>
]

---

## 超越A Bag of Words

* Stop words
* 词性分析

--

* Word order: Markov Model of Order N
    + Unigram: 清华 大学 政治 系
    + Bigram: 清华大学 政治系/清华 大学政治 系
    + Trigram：清华大学政治 系/清华 大学政治系

--

* Weighted words: TF-IDF

???

term frequency-inverse document frequency

--

* Word vector: word2vec

???

 word embeddings

---

## 文本比较

.pull-left[
文本特征：
+ 长度
+ 用词
+ 词频
+ 关键词    
……

]

.pull-right[
语义特征
+ 政治立场
+ 文化
+ 主张
+ 价值
+ 意义
……
]

---

## 文本分析“黑话”

* Corpus
    + Document
    + DTM/TDM

???
Document-term matrix

--

* Classification
    + Dictionary
    + Machine learning
    
--

* Clustering
    + Membership
    + Topic modeling
        + LDA
        + STM
        
???
Latent Dirichlet Allocation

---

## Take Home Points

.pull-left[
### What You Already Know

1. 文本之于社会科学
    + 丰富资源
    + 分析门槛
1. 文本分析路径
    + 词 &dzigrarr; 图
1. 文本分析步骤
    + 打散
    + 聚合

]

--

.pull-right[

### Prospect

* 应用范围广泛
* 间接研究语言
* 不断突破限制


<img src="images/text_indirect_phone.gif" width = 400 />
]


---

class: inverse, center, middle

# Thank you!

`r icon::fa("envelope", size = "lg")`&nbsp; [yuehu@tsinghua.edu.cn](mailto:yuehu@tsinghua.edu.cn)

`r icon::fa("globe", size = "lg")`&nbsp; https://sammo3182.github.io/

`r icon::fa("github", size = "lg")`&nbsp; [sammo3182](https://github.com/sammo3182)