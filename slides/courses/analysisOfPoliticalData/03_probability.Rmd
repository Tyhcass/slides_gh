---
title: "Probability Theory"
subtitle: "Analysis of Political Data (70700173)"
author: "Yue Hu"
institute: "Political Science, Tsinghua University"
# date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: 
      - zh-CN_custom.css
      - styles.css
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

## Overview

+ What's Probability?
    + Three views
+ How To Calculate Probability
    + Two ways
+ Probability vs. Distribution
+ Alternative ways of probability
    + Relative risk
    + Odds
    + Bayesian estimation

---

class: inverse, bottom

# Probability

---

## Probability: A Belief

* Determinist event: Pr = 0/1

--

* Probabilist event: Pr &in; [0, 1]

--

.left-column[<img src="images/tangSeng.jpg" height = 300 />]

.right-column[<img src="images/multiverse_rickMordy.jpg" height = 300 />]

---

## What Dose the "Pr" Mean

--

### Classic ("objective") view:

"Coin has two sides, balance" &rarr; Pr(head) = 0.5.

--

### Relative frequency view 

You are observing a random sample.

$${\displaystyle \lim_{n\to \infty}f(x)/n}$$

(The larger the better

--

### Subjective view

Bayesian, based on beliefs (prior), game theory

---

## How Do We Know the Probability: 

1. Sample Point Method
    + Sample space
    + Sampling replacement
1. Event Composition Method
    + Permutation
    + Combination

---

Sample Space

Space of all possible outcomes

--

Rules:

1. &sum; Pr = 1
1. All of the probability constitute the probability distribution
1. For all event in the space, $P(E) + P(\tilde E) = 1$

???

Two ways to know the probability, but first to know what's sample space is.


---

## Sample Point Method

Get the probability of an event,

--

1. Define experiment event;
1. Define sample space;
1. Assign probabilities, P(E<sub>i</sub>) &GreaterEqual; 0, &sum; P(E<sub>i</sub>) = 1;
1. Define event of interest, A;
1. Find Pr(A) by summing probability of A.

---

## Sampling Replacements

W/o means once picked, it won't be sampled again. 

--

Infinite population (or extremely large sample):

Little difference between sampling with and without replacement). 

---

When the population is finite, the variability of the sample is actually less than expected &rarr;

Finite population correction: Account for this greater efficiency in the sampling process. 

$$ FPC = \sqrt{\frac{N - n}{N - 1}}$$

---

e.g., Through a dice for five times: 

w. replacement: $6^5$

w.o. replacement: $P(6, 5) = \frac{6!}{(6 - 5)!}.$ (n! factorial)

--

Q. Is survey sampling a w. or w.o. sampling? 

???

w.o.

---

## Event Composition Method (ECM)

Determine the probabilities by the application of .red[multiplicative and additive] rules.

--
E<sub>1</sub>: m outcomes, E<sub>2</sub>: n outcomes, therefore there are $m\times n$ possible outcome for E<sub>1</sub>E<sub>2</sub>. 

--

e.g., A car dealer has 6 models, 8 colors, and 3 financial plans. How many package? 

--

6 &times; 8 &times; 3 = 144.

---

## Tools for ECM: Permutation and Combination

Ordered: $P(n, r) = \frac{n!}{(n - r)!}.$

Order doesn't matter: ${n \choose r} = \frac{n!}{r!(n - r)!}.$

---

## Rules for ECM (Compound Event)

Event G and H

* Union: G&cup;H = $\{S_i: S_i\in G\ or\ S_i\in H\}$
    + P(G&cup;H) = P(G) + P(H) - P(G)&cap;P(H).
* Intersection: G&cap;H = $\{S_i: S_i\in G\ and\ S_i\in H\}$
* Exclusive: G&cap;H = &empty;
* Complement: J &sub; -G &cap; H
* Conditional: $P(H|G) = \frac{P(G\cap H)}{P(G)}$
* Independent: P(H|G) = P(H); P(G&cap;H) = P(G)P(H)


---

## Puting Population Together: Distribution

(Probability) distribution: 

The mathematical .red[function] that gives the probabilities of occurrence of different possible .red[outcomes] for an .red[experiment].

--

What probability method is using here?

???

Sample point method. 

--

Is a distribution built upon w. or w/o replacement?

???

Both work.

---

## The Jargon Family of Distribution


### Marginal and joint totals: 

$$\begin{align} 
n_{i\bullet} =& \sum_j n_{ij}; n_{\bullet j} = \sum_i n_{ij}; \\
n_{\bullet\bullet} =& \sum\sum n_{ij}.
\end{align}$$

---

### Marginal and joint distributions

Marginal: 

$$\begin{align}\pi_{i\bullet} =& \sum_j \pi_{ij}, \hat{\pi_{i\bullet}} =  n_{i\bullet}/n_{\bullet\bullet}; \\
\pi_{\bullet j} =& \sum_i \pi_{ij}, \hat{\pi_{\bullet j}} =  n_{\bullet j}/n_{\bullet\bullet}.\end{align}$$

Joint: $\pi_{\bullet\bullet}$ = &sum;&sum; &pi;<sub>ij</sub> = 1.

---

### Conditional distribution

$$\pi_{i|j} = \pi_{ij} / \pi_{\bullet j};$$
$$\hat{\pi_{i|j}} = n_{ij}/n_{\bullet j}$$

---

## Independent Event (Again)

$P(X\cap Y) = P(X)P(Y); P(Y|X) = P(X).$

Such event's probability is: 

* Population: $\pi_i = n_i/N$
* Sample: $\hat{\pi_i} = n_i/n_{\bullet\bullet}.$


---

e.g., Do you think Kamala Harris is a qualified VP? 

| Gender 	| Yes 	| No 	| Total 	|
|--------	|-----	|----	|-------	|
| Male   	| 5   	| 12 	| 17    	|
| Female 	| 6   	| 5  	| 11    	|
| Total  	| 11  	| 17 	| 28    	|

What's the probability for Female who don't agree?

???

Kamala Harris, Senator from California, announced VP in 20120

--

| Gender 	| Yes  	| No    	|
|--------	|------	|-------	|
| Male   	| 5/28 	| 12/28 	|
| Female 	| 6/28 	| 5/28  	|

---

e.g., Do you think Kamala Harris is a qualified VP? 

| Gender 	| Yes 	| No 	| Total 	|
|--------	|-----	|----	|-------	|
| Male   	| 5   	| 12 	| 17    	|
| Female 	| 6   	| 5  	| 11    	|
| Total  	| 11  	| 17 	| 28    	|


Probability of "yes" given the respondent is a male.

--

Conditional: $P(Y|M) = 5/17$

---

e.g., Do you think Kamala Harris is a qualified VP? 

| Gender 	| Yes 	| No 	| Total 	|
|--------	|-----	|----	|-------	|
| Male   	| 5   	| 12 	| 17    	|
| Female 	| 6   	| 5  	| 11    	|
| Total  	| 11  	| 17 	| 28    	|


Are gender and approval independent? 

--

Pr(MY) = .navy[11/28 &times; 17/28] &ne; 5/28

.navy[11/28 &times; 17/28]: .red[Expected value for independence]



???

$\chi^2 = \sum(\frac{Observation - Expected}{Expected})^2.$

---

## Beyond the Chance: Alternative Ways

1. Relative risk
1. Odds Ratio
1. Bayesian method

---

## Relative Risk

* $\hat{\pi}_{1|1}/\hat{\pi}_{1|2} = \frac{n_{11}/n_{\bullet 1}}{n_{12}/n_{\bullet 2}}$

e.g., A hospital investigated whether drinking has a relation with fatal illness. Here's the counts:

| Drinking 	| Fatal 	| Nonfatal 	|
|----------	|-------	|----------	|
| Yes      	| 1,601  	| 162,527   	|
| No       	| 510   	| 412,368   	|

$Risk_{fatal} = \frac{1601/(1601 + 162527)}{510/(510 + 412368)} = 7.897$

???

That is, when people drink, they are 7.9 more times to die in a fatal accident than w/o.

---

## Odds Ratio

$$\begin{align}
O_{12} =& \frac{\pi_{1|1}/\pi_{1|2}}{\pi_{2|1}/\pi_{2|2}} \\
 =& \frac{n_{11}/n_{12}}{n_{21}/n_{22}} \\
 =& n_{11}n_{22}/n_{21}n_{12}
\end{align}$$

---

e.g., A hospital investigated whether drinking has a relation with fatal illness. Here's the counts:

| Seatbelt 	| Fatal 	| Nonfatal 	|
|----------	|-------	|----------	|
| Yes      	| 1,601  	| 162,527   	|
| No       	| 510   	| 412,368   	|

$O_{fatal} = \frac{1601/510}{162527/412368}$ = 7.965


???

The fatal risk of wearing a belt is 8 times larger than not

--

Bonus:  Log odds

???

Log odds, Logit, mapping the range (0,1) to (-&infin;, +&infin;) using log of odds, a.k.a., "logistic unit"


If independent, then log odds = 0.


---

RR vs. OR

.center[<img src="images/rrOr.png" height = 300 />]

* For rare cases, both ok.
* For common cases, RR is better.

---

## Evolutionary View of Probability: Bayes' Theorem

$$
\begin{aligned}
P(A|B)P(B) =& P(B|A)P(A) = P(A\cap B)\\
             P(A|B)=& \frac{P(B|A)P(A)}{P(B)}\\
                   =& \frac{P(B|A)P(A)}{P(B|A)P(B) + P(B|\tilde A)P(\tilde A)}
\end{aligned}
$$

???

1. Conjugate prior: posterior distribution = prior distribution
1. Cromwell's rule: if a region of the parameter space has 0 prior probability, then it also has 0 posterior probability. 
 
---

class: small

e.g.1: Let's assume Idle Fish provides a verification function for buyers based on machine-learning techniques, and .red[30%] of the products were knockoff in the platform. When people bring a product to verify, the system can tell .red[90%] knockoff  knockoff, and .navy[80%] authentic products authentic. Then, what's the probability that a product is knockoff if the verification system said so?

--

.right-column[
$$\begin{align}
& P(K|VK) \\
=& \frac{P(VK|K)P(K)}{P(VK|K)P(K) + P(VK|\tilde K)P(\tilde K)} \\
=& \frac{.9 * .3}{.9 * .3 + .2*.7} = .659.
\end{align}$$
]

--

.left-column[
```{r diagramCar, echo = FALSE}

library("DiagrammeR")

grViz("
digraph usedCar {

  # a 'graph' statement
  graph [overlap = true, fontsize = 10]

  # several 'node' statements
  node [shape = circle,
        fixedsize = true,
        width = 0.9] // sets as circles
  N[label = 'Nature'];
  MF[label = 'Verify'];
  MS[label = 'Verify'];
  RFF[label = 'Result'];
  RFS[label = 'Result'];
  RSF[label = 'Result'];
  RSS[label = 'Result']

  # several 'edge' statements
  N -> MF[label='K: .3']
  N -> MS[label='A: .7']
  MF -> RFF[label='K: .9']
  MF -> RFS[label='A: .1']
  MS -> RSF[label='K: .2']
  MS -> RSS[label='A: .8']
  
}
")

```
]

---

## Why Baysian Estimation?

<video width="700" height="500" controls>
    <source src="images/bayesianApplication.mp4" type="video/mp4">
</video>


---

## Bayesian vs. Frequentist

1. Repeated sample and sample distribution: When the population data are collected, the repeat-sample makes no sense.
1. Future event: It is impossible to have repeat samples for future
